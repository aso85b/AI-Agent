{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aso85b/AI-Agent/blob/main/Hugging_Face_and_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***HuggingFace with LangChain***\n",
        "\n",
        "[Reference 1](https://python.langchain.com/docs/integrations/chat/huggingface/),\n",
        "[2](https://github.com/SonakshiA/HuggingFace-x-LangChain/tree/master)\n",
        "\n",
        "Setup:\n",
        "To access Hugging Face models you'll need to create a Hugging Face account, get an API key, and install the langchain-huggingface integration package."
      ],
      "metadata": {
        "id": "thvKt-KsRqdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMpKMX3eN8Lt"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install --user langchain-community\n",
        "!pip install mistral_inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a Hugging Face token"
      ],
      "metadata": {
        "id": "B3HpqD0OS1Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the token in your script (for testing purposes, avoid hardcoding in production)\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_XXX\"\n",
        "\n",
        "# Retrieve the token later in the script\n",
        "sec_key = os.getenv(\"HF_TOKEN\")\n",
        "print(f\"Token from environment variable: {sec_key}\")\n"
      ],
      "metadata": {
        "id": "l13zHhlBoe_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set environment variable to authenticate. This is done to call models from HuggingFace\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=sec_key"
      ],
      "metadata": {
        "id": "uYWkLVHDUtJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Accessing HuggingFace Models with API***"
      ],
      "metadata": {
        "id": "tNeiCwZFzM5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "hf_token = sec_key\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    temperature=0.7,\n",
        "    model_kwargs={\n",
        "        \"max_length\": 128,\n",
        "        \"token\": hf_token\n",
        "    }\n",
        ")\n",
        "\n",
        "response = llm(\"Can you summarize the following text?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "weLBEQ-IrB-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.7,token=sec_key)"
      ],
      "metadata": {
        "id": "PQXDVZ72U4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "xxB1Q6K2Vdc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is AI?\")"
      ],
      "metadata": {
        "id": "1GYNuZvCWOGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using PromptTemplate\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "question = \"Who won the Cricket World Cup in the year 2011?\"\n",
        "template =\"\"\"Question : {question}\n",
        "Answer: Let's think step by step \"\"\"\n",
        "prompt = PromptTemplate(template=template,input_variables=[\"question\"])\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "8TcI3agjWVu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm,prompt=prompt)\n",
        "print(llm_chain.invoke(question))"
      ],
      "metadata": {
        "id": "u9F5jO0fXIgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***HuggingFacePipeline***"
      ],
      "metadata": {
        "id": "74tf4my_XtMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "eA0c0wtnXvtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ],
      "metadata": {
        "id": "GHVJ6Ss6Yol0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\",tokenizer=tokenizer,max_new_tokens=100)\n",
        "hf=HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "ijZ96KljZ6kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf"
      ],
      "metadata": {
        "id": "r4U2J_6ZaCHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"What is Machine Learning?\")"
      ],
      "metadata": {
        "id": "hkTL4K4IaOEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf.invoke(\"HuggingFace is a company\") #Completes the text"
      ],
      "metadata": {
        "id": "ipCUeBmXa6qa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}